---
title: "Assignment 3"
author: "Group15"
date: "2025-11-06"
output:
  pdf_document:
    latex_engine: xelatex
header-includes:
  - \usepackage{amsmath}
  - \usepackage{enumitem}
  - \usepackage{booktabs}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)

```

## Sarah Dumont
## Sophia Liau  
## Minh Duong

**Question 1**

There are several reason why area of genome with high GC content is hard to sequence:

1.1 Thermal Stability and Secondary Structures

- GC base pairs form three hydrogen bonds compared to the two in AT base pairs. As a result, GC-rich regions have a higher melting temperature ($T_m$) and are more thermally stable. During denaturation and amplification (e.g., in PCR steps of sequencing library preparation), these regions may remain partially double-stranded, leading to incomplete amplification.

- GC-rich sequences tend to form stable secondary structures such as hairpins and G-quadruplexes, which can stall DNA polymerase or cause premature termination of synthesis.

1.2. PCR Amplification Bias

- Most next-generation sequencing (NGS) platforms rely on PCR to amplify DNA fragments before sequencing. GC-rich fragments amplify inefficiently because polymerases can stall or dissociate when encountering secondary structures. This leads to an amplification bias, where GC-rich regions are underrepresented or entirely missing from the sequencing data.

**Question 2**

We want to find the best global alignment between the two following sequences with the provided scoring matrix:

ATTCGAC

ATCAC

Let's first define the sequence, the parameter and create the scoring matrix. We will split the above strings into separate characters, then convert the resulting list into a vector. We will also define the gap penalty to be -2 for every gap occurring in the resulting sequence.

```{r}
seq1 <- unlist(strsplit("ATTCGAC", ""))  # horizontal (rows)
seq2 <- unlist(strsplit("ATCAC", ""))    # vertical (cols)
gap_penalty <- -2

#create scoring matrix from the assignment document
score_matrix <- matrix(
  c( 1, -5, -5, -1,
    -5,  1, -1, -5,
    -5, -1,  1, -5,
    -1, -5, -5,  1),
  nrow = 4, byrow = TRUE,
  dimnames = list(c("A","T","C","G"), c("A","T","C","G"))
)
```

Next, we will make a 2 dimensional grid with an extra row and column for the Needleman-Wunsch Method. Each cell dp[i, j] will stores the best alignment score between the first i lettwers of seq2 and the first j letters of seq1. We also fill the first row and column based on the gap penalty.

```{r}
#initializing the grid
n <- length(seq1)
m <- length(seq2)
dp <- matrix(0, nrow = m + 1, ncol = n + 1)

#fill first row and columns
for (i in 2:(m + 1)) dp[i,1] <- dp[i-1,1] + gap_penalty
for (j in 2:(n + 1)) dp[1,j] <- dp[1,j-1] + gap_penalty
```

We will fill the rest of the grid

```{r}
for (i in 2:(m + 1)) {
  for (j in 2:(n + 1)) {
    match_score <- score_matrix[seq2[i-1], seq1[j-1]]
    dp[i,j] <- max(
      dp[i-1,j-1] + match_score,  # diagonal (match/mismatch)
      dp[i-1,j] + gap_penalty,    # up (gap in seq1)
      dp[i,j-1] + gap_penalty     # left (gap in seq2)
    )
  }
}

#print the table
dp
dp[1:3, 1:3]
```

Now we will traceback to find the best alignment. Starting from the bottom-right cell, the traceback reconstructs how the optimal alignment was formed. If the score came from the diagonal, both letters were aligned. If from up, a gap was added in seq1. If from left, a gap was added in seq2. The loop continues until both i and j reach 1 (the top-left corner).

```{r}
i <- m + 1
j <- n + 1
align1 <- c()
align2 <- c()

while (i > 1 || j > 1) {
  if (i > 1 && j > 1 && dp[i,j] == dp[i-1,j-1] + score_matrix[seq2[i-1], seq1[j-1]]) {
    align1 <- c(seq1[j-1], align1)
    align2 <- c(seq2[i-1], align2)
    i <- i - 1
    j <- j - 1
  } else if (i > 1 && dp[i,j] == dp[i-1,j] + gap_penalty) {
    align1 <- c("-", align1)
    align2 <- c(seq2[i-1], align2)
    i <- i - 1
  } else {
    align1 <- c(seq1[j-1], align1)
    align2 <- c("-", align2)
    j <- j - 1
  }
}

cat("Best alignment:\n")
cat(paste(align1, collapse = ""), "\n")
```

**Question 3**

3.1. We want to load the first 73 lines of the header of the file and print the contents

```{r}
df <- read.csv("single_cell_RNA_seq_bam.sam", nrows=73, sep="\t", header=FALSE, fill=TRUE)
```

According to the header table in section 1.3 of the BAM/SAM document in the appendix, we have:

- SN tag: this is the reference sequence name

- LN tag: this is reference sequence length that ranges from 1 to $2^{31}-1$

3.2. The length of the X chromosome, in bp, for our alignment is:

```{r}
#Find the row where SN is "X"
x_row <- df[grepl("SN:X", df$V2), ]

#Extract the length in base pairs
x_length <- sub("LN:", "", x_row$V3)
x_length <- as.numeric(x_length)

x_length
```

**Question 4**

4.1. The number of reads in this BAM file is:

```{r}
sam <- read.csv("single_cell_RNA_seq_bam.sam", sep="\t", header=FALSE,
                comment.char="@", col.names = paste0("v", seq_len(30)), fill=TRUE)
sam <- sam[paste0("v", seq_len(11))]
nrow(sam)
```

4.2. Let's first print out the 10th row of a dataframe to look at the format of a read

```{r}
sam[10, ]
```
According to section 1.4 of the BAM documentation, we can see that to find the chromosone to which the read was aligned, we should look at column 3, which is the reference sequence name. We can see that this is chromosome 1. This is the \textbf{V3} column. The V11 column correspond to \textbf{base quality scores (ASCII-encoded)}.

4.3. The number of reads correspond to chromosome X is:

```{r}
numX <-sum(sam$v3 == "X")
numX
```

4.4. The base quality string is in V11 (QUAL field) and each character encode a quality score using Phred+33 encoding. We will convert this score to a numeric score, then average them for all reads aligned to X

```{r}
#Subset reads aligned to chromosome X
x_reads <- sam[sam$v3 == "X", ]

#Convert QUAL strings to numeric BQ and compute the mean
bq_values <- unlist(lapply(x_reads$v11, function(q) as.integer(charToRaw(q)) - 33))
mean_bq_x <- mean(bq_values, na.rm = TRUE)
cat("The average BQ scores for all read aligned to chromosome X is: ")
cat(paste(mean_bq_x, collapse = ""), "\n")
```

4.5. We want to plot the distribution of BQs across all bases and reads as a boxplot. We will first convert all QUAL fields to Phred scores

```{r}
qual_list <- lapply(sam$v11, function(q) utf8ToInt(q) - 33)
max_len <- max(sapply(qual_list, length))
max_len

# Pad all reads with NA so each read has equal length
qual_mat <- t(sapply(qual_list, function(x) c(x, rep(NA, max_len - length(x)))))
```
We see that the maximum read length is 58, meaning the reads in the SAM/BAM file are only 58 bp long. Now we will create a list of Phred scores for each base position, for example: 

bq_list[[1]] = qualities at position 1 across all reads

```{r}
bq_list <- lapply(1:max_len, function(i) qual_mat[, i])

names(bq_list) <- paste0(1:max_len)
```

Now we can plot the distribution of basequality across all bases and reads
```{r}
boxplot(bq_list,
        las = 2,
        col = "skyblue",
        border = "navy",
        main = "Base Quality Distribution",
        xlab = "Chromosome",
        ylab = "Base Quality",
        outline = TRUE) 
```
The base quality distribution shows a high median Phred score of around 35 to 36 for all positions, indicating great sequencing accuracy (>99.9%). Most bases fall within the Q30–Q37 range, with only a few low-quality outliers below Q25. This suggests that the dataset is of high overall quality, and downstream analyses can be performed with confidence.

4.6. Section 1.4 of the SAM/BAM documentation indicates V4 as the leftmost mapping position of the reads

4.7. We want to find reads have their leftmost mapping position aligned within bases 40801273 - 40805199 at chromosome 9 which encode protein Hspa8.

```{r}
#Filter reads from chromosome 9 and within the provided coordinates
hspa8_reads <- sam[sam$v3 == "9" & sam$v4 >= 40801273 & sam$v4 <= 40805199, ]
nrow(hspa8_reads)
```

4.8.  Section 1.4 of the SAM/BAM documentation indicates V5 as the mapping quality. The number of reading iwth mapping quality less than 50 is:

```{r}
sum(sam$v5 < 50)
```

4.9. The mean mapping quality of the reads which have mapping quality less than 50 is:
```{r}
mean(sam$v5[sam$v5 < 50])
```

4.10. The number of reads which align to the tdTomato sequence is:

```{r}
sum(sam$v3 == "tdTomato")
```

We see that the cell expresses tdTomato, so it should emit fluorescence under appropriate light. Fluorophore tags can help researchers with visually track gene expression or identify specific cell types under a microscope, separate cell populations using fluorescence.

**Question 5**

5.1. Let's first obtain the header of the file and a dataframe where each row is a variant.

```{r}
vcf_con <- file("RNA_seq_annotated_variants.vcf", open="r")
vcf_file <- readLines(vcf_con)
close(vcf_con)
vcf <- data.frame(vcf_file)
header <- vcf[grepl("##", vcf$vcf_file), ]
factor(header)
variants <- read.csv("RNA_seq_annotated_variants.vcf", skip=length(header),
header=TRUE, sep="\t")
```

Let's look at the variants dataframe.

```{r}
colnames(variants)
```
We see that this is consistent with the Variant Call Format standard. Let's extract the first row's REF and ALT alleles which is the reference allele base and the alternative allele:

```{r}
variants[1, ]
ref_allele <- variants$REF[1]
alt_allele <- variants$ALT[1]
ref_allele
alt_allele
```
We see that the reference allele is G and the alternative allele called by Strelka is A.

5.2. We want to obtain the entirety of the ANN info value contents from the INFO field for the first variant.

```{r}
# Get the INFO field of the first variant and convert to string format
info_field <- as.character(variants$INFO[1])

# Split by semicolon
info_split <- strsplit(info_field, ";")[[1]]

# Find the entry that starts with "ANN="
ann_entry <- info_split[grep("^ANN=", info_split)]

# Extract the value part after "ANN="
ann_value <- sub("^ANN=", "", ann_entry)
ann_value
```

5.3. From the headers from the first part of the question, we have the following field for the format of ANN value content: ##INFO=<ID=ANN,Number=.,Type=String,Description="Functional annotations: 'Allele | Annotation | Annotation_Impact | Gene_Name | Gene_ID | Feature_Type | Feature_ID | Transcript_BioType | Rank | HGVS.c | HGVS.p | cDNA.pos / cDNA.length | CDS.pos / CDS.length | AA.pos / AA.length | Distance | ERRORS / WARNINGS / INFO' ">

The Annotation field is the second field, we can extract this:
```{r}
# Create a vector with each ANN field
ann_split <- strsplit(ann_value, "\\|")[[1]]
annotation_type <- ann_split[2]
annotation_type
```

This tells us that the mutation occur in the intron region (non-coding) of a gene.

5.4. We can do the same process for variant 683

```{r}
# Extract the INFO field of variant 683
info_field_683 <- as.character(variants$INFO[683])
info_split_683 <- strsplit(info_field_683, ";")[[1]]
ann_entry_683 <- info_split_683[grep("^ANN=", info_split_683)]
ann_value_683 <- sub("^ANN=", "", ann_entry_683)

```

Similar to 5.3, we will create a vector with each ANN field, the extract the gene name, which is the 4th field indicated in the header section.

```{r}
# Create a vector with each ANN field
ann_first_683 <- strsplit(ann_value_683, ",")[[1]][1]
ann_split_683 <- strsplit(ann_first_683, "\\|")[[1]]

# Extract gene name
gene_683 <- ann_split_683[4]
gene_683
```

5.5. We will construct a function to split the INFO fields at ";" then find the ANN tag. Then remove "ANN=" part which leaves us with only ANN fields of all variants. ann_first corresponds to the first snpEff annotation for each variant. The function will return the annotation type which is the second field of the ANN entry. Using sapply, we will repeat for all the ANN fields.

```{r}
# Extract all ANN fields
info_all <- as.character(variants$INFO)

# Get all ANN entries
ann_all <- sapply(info_all, function(x) {
  ann_tag <- strsplit(x, ";")[[1]]
  ann_entry <- ann_tag[grep("^ANN=", ann_tag)]
  if (length(ann_entry) == 0) return(NA)
  ann_value <- sub("^ANN=", "", ann_entry)
  ann_first <- strsplit(ann_value, ",")[[1]][1]
  strsplit(ann_first, "\\|")[[1]][2]  # annotation type
})
```

We can see the variants type count in the table below:

```{r}
# Count by variant type
table(ann_all)
sum(table(ann_all))
```

5.6. A frameshift variant is an insertion or deletion (indel) whose length is not a multiple of 3. This shifts the triplet reading frame of the mRNA during translation and causes all downstream codons to be read incorrectly. Missense variants only change one amino acid, but a frameshift variant changes all downstream amino acids in translation, so it has a more negative effect.

5.7. We can use grepl() on the INFO field to find those containing "intron_variant". Then we will compare it with the total number of variants.

```{r}
intronic_variants <- sum(grepl("intron", variants$INFO))
intergenic_variants <- sum(grepl("intergenic", variants$INFO))
total_in <- sum(intronic_variants,intergenic_variants)

# Total number of variant calculation
n_total <- nrow(variants)
percentage_intronic <- (total_in / n_total) * 100
percentage_intronic
```

We see that around 83.37% of variants are intronic/intergenic which is consistent since most of the genomic DNA are non-coding.

5.8. In the INFO header, we can see that the 3rd field of ANN outline the impact of a variant. We will repeat what we did in 5.5 but this time the function will return the first snpEff annotation for each variant.

```{r}
ann_firsts <- sapply(info_all, function(x) {
  ann_tag <- strsplit(x, ";")[[1]]
  ann_entry <- ann_tag[grep("^ANN=", ann_tag)]
  if (length(ann_entry) == 0) return(NA)
  ann_value <- sub("^ANN=", "", ann_entry)
  ann_first <- strsplit(ann_value, ",")[[1]][1]
})

# Get components
ann_split_all <- strsplit(ann_firsts, "\\|")
```

Let's create a data frame with only information we are interested in, which is Annotation, Impact and Gene, their position can be found in the INFO header. 

```{r}
# Create data frame containing the gene, type of variant and impact
ann_df <- data.frame(
  Annotation = sapply(ann_split_all, `[`, 2),
  Impact = sapply(ann_split_all, `[`, 3),
  Gene = sapply(ann_split_all, `[`, 4)
)
```

Coding variant will have specific keywords, we want only those variants. Then we can filter out the HIGH impact coding variant. Note that we also define splice variants as coding variants despite them being on an intron. This is because it affects how the exons is joined together which ultimately allows them to have the same impact as a coding variant.

```{r}
coding_annots <- c(
  "missense",
  "stop",
  "start",
  "frameshift",
  "inframe",
  "splice",
  "coding",
  "synonymous"
)

high_coding <- ann_df[
  grepl("HIGH", ann_df$Impact) &
  grepl(paste(coding_annots, collapse="|"), ann_df$Annotation),
]

unique(high_coding$Gene)
table(high_coding$Impact)
```

There are only 4 genes that have HIGH impact variant. Let's go one step further and see these protein's function, specifically those that are in the exon region:

- Rps14: Encodes ribosomal protein S14, a component of the 40S ribosomal subunit.

- Rps19: Encodes ribosomal protein S19, also part of the 40S subunit.

- Hnrnpl: Encodes heterogeneous nuclear ribonucleoprotein L, an RNA-binding protein that regulates pre-mRNA splicing and stability.

We see that a HIGH impact mutation on Rps19 can cause Diamond-Blackfan anemia, affecting erythropoiesis (red-blood-cell formation), so it makes sense for these kind of mutations to be so rare. 

5.9. Insertions larger than the read length (60 bp) can’t be properly aligned or reconstructed, so Strelka cannot detect them reliably.

5.10. According to section 5 of the VCF documentation, we are interested in the second last and the last column where we can see the order of the tags and their associated value. Let's first extract the tags column which will have the format similar toGT:AD:DP:GQ:PL

```{r}
tag_col <- variants [, ncol(variants)-1]
head(tag_col)
```

Let's also look at the last column
```{r}
# Extract genotype field for all variants
geno_field <- variants[, ncol(variants)]  # usually last column
head(geno_field)
```

We will first write a function to calculate VAF which takes in the tag column and the field column. It will find the AD (allele depth) values then extract the REF and ALT read counts. The VAF can then be calculated by taking:

\[\frac{x}{x+y}\]

where $x$ is the ALT reads and $y$ is the REF read.

```{r}
library(ggplot2)

# Function to compute VAF
get_vaf <- function(entry_fmt, entry_sample) {

  # Split tag colum and fields
  fmt_fields  <- unlist(strsplit(entry_fmt, ":"))
  samp_fields <- unlist(strsplit(entry_sample, ":"))

  # Locate AD field by name
  ad_idx <- which(fmt_fields == "AD")
  if (length(ad_idx) == 0) return(NA)

  # Extract AD counts
  ad_str  <- samp_fields[ad_idx]
  counts  <- as.numeric(unlist(strsplit(ad_str, ",")))

  # Must have REF, ALT
  if (length(counts) != 2) return(NA)

  ref <- counts[1]
  alt <- counts[2]
  total <- ref + alt
  if (total == 0) return(NA)

  alt / total
}
```

Next, we will apply the function get_vaf to every variants. 
```{r}
# Apply to all variants
tag_col <- variants[, ncol(variants)-1]    # FORMAT column
geno_field <- variants[, ncol(variants)] 
vafs <- mapply(get_vaf, tag_col, geno_field)
```

Now we will use the list of annotation type that was produced earlier. We will group VAFs by annotation type then plot the distribution of VAFs across all variant type.
```{r}
# Group VAFs by annotation type
vaf_list <- split(vafs, ann_all)
names(vaf_list) <- substr(names(vaf_list), 1, 6)

# Plot
par(mar = c(10, 4, 2, 2))
boxplot(vaf_list, las=2, col="lightblue",
        main="VAF per Variant", ylab="Variant Allele Frequency")

```
Next, we need to find the variants that have VAF greater than 5%.
```{r}
# Count variants with VAF > 5%
vaf_above_5 <- sum(vafs > 0.05, na.rm = TRUE)
vaf_above_5
```
Then, we will find the coding variants with VAF greater than 5%. We will first use ann_df which has the following structure:
```{r}
colnames(ann_df)
```

We will add a column with the VAF values of each variant, then use coding_keywords to filter for coding variant, then taking only those with VAF > 0.05

```{r}
# Combine VAF and annotation dataframe
ann_df$VAF <- vafs

coding_variants <- ann_df[
  grepl(paste(coding_annots, collapse="|"), ann_df$Annotation), ]

coding_vaf_above_5 <- sum(coding_variants$VAF > 0.05, na.rm = TRUE)
coding_vaf_above_5
```

## Contribution

All group members did the assignment individually before discussing results. These versions can be seen on github repo: https://github.com/hnimoaht/BMEG310-Assignment-3-ver. There are some discrepencies in the version, particularly in the boxplot sections and finding coding variants with VAF above 0.05. Sarah Dumont's boxplot were used in this final draft. We debugged 5.10 together as a group and reaches a common result. Minh Duong ensures consistency in the final draft and compiles code. 











